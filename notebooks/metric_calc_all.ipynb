{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalseg\n",
    "from evalseg.metrics import MME,MultiClassMetric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interact_manual, IntSlider\n",
    "import os,glob\n",
    "import auto_profiler\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = '../datasets'\n",
    "only_2d_slice=True\n",
    "out_root='../out'\n",
    "os.makedirs(out_root,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(num_classes)->evalseg.metrics.MultiMetric:\n",
    "    metrics = {\n",
    "        # 'mme': evalseg.metrics.MME(),\n",
    "        'hd': evalseg.metrics.HD(),\n",
    "        'voxel': evalseg.metrics.Voxel(),\n",
    "        'nsd t=1': evalseg.metrics.NSD(tau=1),\n",
    "        'nsd t=5': evalseg.metrics.NSD(tau=5),\n",
    "\n",
    "        # evalseg.metrics.BD,\n",
    "    }\n",
    "    from evalseg.metrics import MultiClassMetric, MultiMetric\n",
    "    return MultiMetric(metrics_dic={m: MultiClassMetric(metrics[m], num_classes)for m in metrics})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Metric over all datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalseg.reload()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with auto_profiler.Profiler(filterExternalLibraries=False,depth=20):\n",
    " for dataset_name in sorted([d for d in os.listdir(root_data) if os.path.isdir(f'{root_data}/{d}')]):\n",
    "    # if 'Livre' in dataset_name:\n",
    "    #     continue\n",
    "    dataset = evalseg.io.Dataset(f'{root_data}/{dataset_name}')\n",
    "    for case in sorted(dataset.get_available_ids()):\n",
    "        out_path=f'{out_root}/{dataset_name}-{case}'\n",
    "        if len(glob.glob(f'{out_path}-*')) >= (3 if only_2d_slice else 1):\n",
    "            continue\n",
    "\n",
    "        # dataset.load_all_of_case(case)\n",
    "        gto = dataset.get_groundtruth(case)\n",
    "        # cto = dataset.get_CT(case)\n",
    "        # if cto is None:\n",
    "        #     continue\n",
    "\n",
    "        metric = get_metrics(dataset.num_labels)\n",
    "        \n",
    "        metric.set_reference(gto)\n",
    "        preds = {p: dataset.get(p, case) for p in dataset.get_prediction_methods()}\n",
    "        res = metric.evaluate_multi(preds,parallel=0)\n",
    "        print(res)\n",
    "        with open(f'{out_path}.json', 'w') as f:\n",
    "            json.dump(res,f,indent=4)\n",
    "        \n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalseg\n",
    "evalseg.metrics.MME.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Max Slice (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalseg.reload()\n",
    "# with auto_profiler.Profiler(filterExternalLibraries=False,depth=10):\n",
    "for dataset_name in sorted([d for d in os.listdir(root_data) if os.path.isdir(f'{root_data}/{d}')]):\n",
    "    if 'Livre' in dataset_name:continue\n",
    "    dataset = evalseg.io.NibDataset(f'{root_data}/{dataset_name}')\n",
    "    for case in sorted(dataset.get_available_ids()):\n",
    "        if len(glob.glob(f'{out_root}/{dataset_name}-{case}-*')) >= (3 if only_2d_slice else 1):\n",
    "            continue\n",
    "\n",
    "        # dataset.load_all_of_case(case)\n",
    "        gto, gto_voxelsize = dataset.get_groundtruth(case)\n",
    "        cto, cto_voxelsize = dataset.get_CT(case)\n",
    "        if cto is None: continue\n",
    "        \n",
    "        if only_2d_slice:\n",
    "            for c in range(1, dataset.num_labels):\n",
    "                gto_tmp=gto == c\n",
    "                gtmax = (gto_tmp.sum(axis=2).sum(axis=1).argmax(), gto_tmp.sum(axis=2).sum(axis=0).argmax(), gto_tmp.sum(axis=1).sum(axis=0).argmax())\n",
    "                axes = ['x', 'y', 'z'] \n",
    "                for ax in axes:\n",
    "                    axi = {'all': -1, 'x': 0, 'y': 1, 'z': 2}[ax]\n",
    "                    frame = -1 if ax == 'all' else gtmax[axi]\n",
    "                    \n",
    "                    gt, voxelsize = evalseg.geometry.slice(gto, gto_voxelsize, axi, [frame])\n",
    "                    ct, _ = evalseg.geometry.slice(cto, None, axi, [frame])\n",
    "                    preds = {p: evalseg.geometry.slice(dataset.get_prediction(p, case)[0], None, axi, [frame])[0] == c\n",
    "                            for p in dataset.get_prediction_methods()}\n",
    "                    preds = {'GroundTruth': gt, **preds}\n",
    "\n",
    "                    out_path = f'{out_root}/{dataset_name}-{case}'\n",
    "                    \n",
    "                    out_path = f'{out_path}-{ax}-{frame}-for{c}'\n",
    "                    ctlbl = f\"{dataset_name.split('_')[1]} {case} {ax}={frame}\"\n",
    "                    evalseg.ui.multi_plot_2d(ct, gt,  preds,\n",
    "                                            spacing=voxelsize, col=6, ctlbl=ctlbl, z_titles=[frame],\n",
    "                                            dst=f'{out_path}-allct.png', show=False)\n",
    "                    \n",
    "                    print(out_path)\n",
    "\n",
    "                    # mme = evalseg.metrics.MultiClassMetric(evalseg.metrics.MME, dataset.num_labels)\n",
    "                    metric = get_metrics(dataset.num_labels)\n",
    "        \n",
    "                    metric.set_reference(gto, gto_voxelsize)\n",
    "                    \n",
    "                    res = metric.evaluate_multi(preds,parallel=0)\n",
    "                    print(res)\n",
    "                    with open(f'{out_path}.json', 'w') as f:\n",
    "                        json.dump(res,f,indent=4)\n",
    "                    \n",
    "                    k = list(res['mme'].keys())[0]\n",
    "                    res = {'ignore it a': res['mme'][k], 'ignore it b': res['mme'][k], **res}\n",
    "                    evalseg.ui.plot_metric_multi(res['mme'], name=dataset_name, dst=f'{out_path}-metric{c}.png', show=False, col=6)\n",
    "                    \n",
    "                    evalseg.ui.img.concat(glob.glob(f'{out_path}-[!res]*.png'), out=f'{out_path}-res-{c}.png')\n",
    "                    [os.remove(f) for f in glob.glob(f'{out_path}-[!res]*.png')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalseg.reload()\n",
    "with auto_profiler.Profiler(filterExternalLibraries=False,depth=10):\n",
    "    metric = get_metrics(dataset.num_labels)\n",
    "            \n",
    "    metric.set_reference(gt, voxelsize)\n",
    "    res = metric.evaluate_multi(preds,parallel=0)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalseg.reload()\n",
    "res = metric.evaluate_multi({'CE':preds['CE']},parallel=0)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str({'sadsadsad':np.zeros(10)})[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GroundTruth'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
